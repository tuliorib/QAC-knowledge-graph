{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ekAhd5406vo8",
        "outputId": "542c641a-c443-49f4-b51f-6e6db41958a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.44.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.44.1-py3-none-any.whl (373 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.5/373.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.44.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload a PDF file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bc7f661c-f926-4387-8ff5-9b4a7fbc6b03\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bc7f661c-f926-4387-8ff5-9b4a7fbc6b03\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving graham.pdf to graham.pdf\n",
            "Processing graham.pdf...\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Processing complete. Results saved to graham_processed.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_044b84ab-468e-4f6a-9aa4-3f80f50d1e66\", \"graham_processed.json\", 15981)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison Results:\n",
            "--------------------------------------------------\n",
            "Query: What is the main topic of the document?\n",
            "Chunked DB - Top 5 results:\n",
            "Chunk 0 (Score: 1.4893):\n",
            "Concepts: ['Founder Mode', 'Manager Mode', 'Leadership']\n",
            "Key Sentences: [\"The theme of Brian's talk was that the conventional wisdom about how to run larger companies is mistaken.\", 'He had to figure out a better way on his own, which he did partly by studying how Steve Jobs ran Apple.']\n",
            "Chunk 5 (Score: 1.5068):\n",
            "Concepts: ['Founder Mode', 'Delegation', 'Company Growth']\n",
            "Key Sentences: [\"So is it a good idea, or a bad one? We still don't know.\", 'Founder mode will be more complicated than manager mode.']\n",
            "Chunk 7 (Score: 1.5189):\n",
            "Concepts: ['Founder Mode', 'CEO Impact', 'Modular Approach']\n",
            "Key Sentences: ['The modular approach does at least limit the damage a bad CEO can do.', 'Thanks to Brian Chesky, Patrick Collison, Ron Conway, Jessica Livingston, Elon Musk, Ryan Petersen, Harj Taggar, and Garry Tan for reading drafts of this.']\n",
            "Chunk 1 (Score: 1.5309):\n",
            "Concepts: ['Founder Mode', 'Manager Mode', 'Business Strategy']\n",
            "Key Sentences: [\"Airbnb's free cash flow margin is now among the best in Silicon Valley.\", \"What they were being told was how to run a company you hadn't founded, how to run a company if you're merely a professional manager.\"]\n",
            "Chunk 3 (Score: 1.5448):\n",
            "Concepts: ['Founder Mode', 'Manager Mode', 'Micromanagement']\n",
            "Key Sentences: ['What this often turns out to mean is: hire professional fakers and let them drive the company into the ground.', \"Founders feel like they're being gaslit from both sides by the people telling them they have to run their companies like managers.\"]\n",
            "\n",
            "Full-text DB - Top 5 results:\n",
            "Sentence 4 (Score: 1.4460): Instead I want to talk about a question it raised.\n",
            "Sentence 36 (Score: 1.5331): One theme I noticed both in Brian's talk and when talking to founders afterward\n",
            "was the idea of being gaslit.\n",
            "Sentence 5 (Score: 1.5694): The theme of Brian's talk was that the conventional wisdom about how to run\n",
            "larger companies is mistaken.\n",
            "Sentence 71 (Score: 1.5777): Thanks  to Brian Chesky , Patrick Collison, Ron Conway , Jessica Livingston, Elon\n",
            "Musk, R yan Petersen, Harj Taggar , and Garry Tan for reading drafts of this.9/10/24, 8:53 AM Founder Mode\n",
            "https://paulgraham.com/foundermode.html 3/3\n",
            "Sentence 66 (Score: 1.5904): [2] If the practice of having such retreats became so widespread that even mature\n",
            "companies dominated by politics started to do it, we could quantify the senescence\n",
            "of companies by the average depth on the or g chart of those invited.\n",
            "--------------------------------------------------\n",
            "Query: Who are the key people mentioned?\n",
            "Chunked DB - Top 5 results:\n",
            "Chunk 0 (Score: 1.2346):\n",
            "Concepts: ['Founder Mode', 'Manager Mode', 'Leadership']\n",
            "Key Sentences: [\"The theme of Brian's talk was that the conventional wisdom about how to run larger companies is mistaken.\", 'He had to figure out a better way on his own, which he did partly by studying how Steve Jobs ran Apple.']\n",
            "Chunk 6 (Score: 1.3197):\n",
            "Concepts: ['Founder Mode', 'Eccentricity in Leadership', 'Misuse of Concepts']\n",
            "Key Sentences: [\"Imagine what they'll do once we can tell them how to run their companies like Steve Jobs instead of John Sculley.\", 'As soon as the concept of founder mode becomes established, people will start misusing it.']\n",
            "Chunk 3 (Score: 1.3314):\n",
            "Concepts: ['Founder Mode', 'Manager Mode', 'Micromanagement']\n",
            "Key Sentences: ['What this often turns out to mean is: hire professional fakers and let them drive the company into the ground.', \"Founders feel like they're being gaslit from both sides by the people telling them they have to run their companies like managers.\"]\n",
            "Chunk 7 (Score: 1.3355):\n",
            "Concepts: ['Founder Mode', 'CEO Impact', 'Modular Approach']\n",
            "Key Sentences: ['The modular approach does at least limit the damage a bad CEO can do.', 'Thanks to Brian Chesky, Patrick Collison, Ron Conway, Jessica Livingston, Elon Musk, Ryan Petersen, Harj Taggar, and Garry Tan for reading drafts of this.']\n",
            "Chunk 4 (Score: 1.3393):\n",
            "Concepts: ['Founder Mode', 'Management Practices', 'Leadership Engagement']\n",
            "Key Sentences: [\"VCs who haven't been founders themselves don't know how founders should run companies.\", \"Skip-level meetings will become the norm instead of a practice so unusual that there's a name for it.\"]\n",
            "\n",
            "Full-text DB - Top 5 results:\n",
            "Sentence 44 (Score: 1.2223): For example, Steve Jobs used to run an annual retreat for what he considered the\n",
            "100 most important people at Apple, and these were not the 100 people highest on\n",
            "the or g chart.\n",
            "Sentence 71 (Score: 1.2371): Thanks  to Brian Chesky , Patrick Collison, Ron Conway , Jessica Livingston, Elon\n",
            "Musk, R yan Petersen, Harj Taggar , and Garry Tan for reading drafts of this.9/10/24, 8:53 AM Founder Mode\n",
            "https://paulgraham.com/foundermode.html 3/3\n",
            "Sentence 64 (Score: 1.3027): Notes\n",
            "[1] The more diplomatic way of phrasing this statement would be to say that\n",
            "experienced C-level execs are often very skilled at managing up.\n",
            "Sentence 14 (Score: 1.3318): Why was everyone telling these founders the wrong thing?\n",
            "Sentence 12 (Score: 1.3616): The audience at this event included a lot of the most successful founders we've\n",
            "funded, and one after another said that the same thing had happened to them.\n",
            "--------------------------------------------------\n",
            "Query: What are the main conclusions or findings?\n",
            "Chunked DB - Top 5 results:\n",
            "Chunk 1 (Score: 1.4827):\n",
            "Concepts: ['Founder Mode', 'Manager Mode', 'Business Strategy']\n",
            "Key Sentences: [\"Airbnb's free cash flow margin is now among the best in Silicon Valley.\", \"What they were being told was how to run a company you hadn't founded, how to run a company if you're merely a professional manager.\"]\n",
            "Chunk 0 (Score: 1.4893):\n",
            "Concepts: ['Founder Mode', 'Manager Mode', 'Leadership']\n",
            "Key Sentences: [\"The theme of Brian's talk was that the conventional wisdom about how to run larger companies is mistaken.\", 'He had to figure out a better way on his own, which he did partly by studying how Steve Jobs ran Apple.']\n",
            "Chunk 6 (Score: 1.5294):\n",
            "Concepts: ['Founder Mode', 'Eccentricity in Leadership', 'Misuse of Concepts']\n",
            "Key Sentences: [\"Imagine what they'll do once we can tell them how to run their companies like Steve Jobs instead of John Sculley.\", 'As soon as the concept of founder mode becomes established, people will start misusing it.']\n",
            "Chunk 3 (Score: 1.5416):\n",
            "Concepts: ['Founder Mode', 'Manager Mode', 'Micromanagement']\n",
            "Key Sentences: ['What this often turns out to mean is: hire professional fakers and let them drive the company into the ground.', \"Founders feel like they're being gaslit from both sides by the people telling them they have to run their companies like managers.\"]\n",
            "Chunk 2 (Score: 1.5678):\n",
            "Concepts: ['Founder Mode', 'Manager Mode', 'Business Management']\n",
            "Key Sentences: ['There are two different ways to run a company: founder mode and manager mode.', 'I hope in a few years founder mode will be as well understood as manager mode.']\n",
            "\n",
            "Full-text DB - Top 5 results:\n",
            "Sentence 25 (Score: 1.4512): All we have so far are the experiments of individual\n",
            "founders who've been figuring it out for themselves.\n",
            "Sentence 50 (Score: 1.5421): So is it a good idea, or a bad one?\n",
            "Sentence 61 (Score: 1.5423): [3]\n",
            "Curiously enough it's an encouraging thought that we still know so little about\n",
            "founder mode.\n",
            "Sentence 36 (Score: 1.5543): One theme I noticed both in Brian's talk and when talking to founders afterward\n",
            "was the idea of being gaslit.\n",
            "Sentence 15 (Score: 1.5632): That was the big\n",
            "mystery to me.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install faiss-cpu\n",
        "!pip install scikit-learn\n",
        "\n",
        "import os\n",
        "import json\n",
        "import nltk\n",
        "from typing import List, Dict\n",
        "from google.colab import files\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sklearn.preprocessing import normalize\n",
        "import re\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set up OpenAI client (replace with your actual API key)\n",
        "client = OpenAI(api_key=userdata.get('openai_key'))\n",
        "\n",
        "# Reuse functions from the previous script\n",
        "def upload_file():\n",
        "    \"\"\"Upload a file to Google Colab.\"\"\"\n",
        "    uploaded = files.upload()\n",
        "    return next(iter(uploaded))\n",
        "\n",
        "def read_pdf(file_path: str) -> str:\n",
        "    \"\"\"Read text from a PDF file.\"\"\"\n",
        "    # Note: In Colab, you might need to install PyPDF2 or another PDF reader\n",
        "    !pip install PyPDF2\n",
        "    import PyPDF2\n",
        "\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean the extracted text.\"\"\"\n",
        "    # Replace Unicode characters with their ASCII equivalents\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def split_into_sentences(text: str) -> List[str]:\n",
        "    \"\"\"Split the text into sentences.\"\"\"\n",
        "    return nltk.sent_tokenize(text)\n",
        "\n",
        "def create_chunks(sentences: List[str], chunk_size: int = 10) -> List[str]:\n",
        "    \"\"\"Create fixed-size chunks from sentences.\"\"\"\n",
        "    chunks = []\n",
        "    for i in range(0, len(sentences), chunk_size):\n",
        "        chunk = ' '.join(sentences[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def get_initial_context(text: str) -> Dict:\n",
        "    \"\"\"Get initial context for the entire document.\"\"\"\n",
        "    prompt = f\"\"\"Analyze the following text and provide:\n",
        "    1. Main topics (abstract concepts, themes, or subject areas)\n",
        "    2. Key entities (specific people, companies, organizations, or locations)\n",
        "    3. Overall sentiment\n",
        "    4. Brief summary\n",
        "\n",
        "    Ensure that the main topics and key entities are distinct categories without overlap.\n",
        "\n",
        "    Text: {text[:10000]}  # Limiting to first 10000 characters\n",
        "\n",
        "    Respond in JSON format. You are part of a bigger system, and you must always respond with a pure JSON response. The system will break if you don't. NEVER add a JSON code block syntax to your response\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that analyzes text and extracts key information.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    return json.loads(response.choices[0].message.content)\n",
        "\n",
        "\n",
        "def process_chunk(chunk: str, global_context: Dict, previous_chunks_metadata: List[Dict]) -> Dict:\n",
        "    \"\"\"Process a single chunk and extract metadata.\"\"\"\n",
        "    prompt = f\"\"\"Analyze the following text chunk and provide metadata according to the specified format.\n",
        "    Use the provided global context and previous chunk metadata for consistency.\n",
        "\n",
        "    Global Context: {json.dumps(global_context)}\n",
        "\n",
        "    Previous Chunks Metadata: {json.dumps(previous_chunks_metadata[-3:] if previous_chunks_metadata else [])}\n",
        "\n",
        "    Text Chunk: {chunk}\n",
        "\n",
        "    Provide the following metadata in JSON format. Ensure that each category is distinct and there is no overlap between them:\n",
        "\n",
        "    1. concepts: Abstract ideas or themes discussed in the text (max 3)\n",
        "    2. subjects: Specific areas or fields of study mentioned (max 3)\n",
        "    3. topics: Particular issues or matters being discussed (max 3)\n",
        "    4. people: Names of individuals mentioned\n",
        "    5. dates: Any dates or time periods referenced\n",
        "    6. organizations: Names of companies, institutions, or groups\n",
        "    7. locations: Any places or geographical areas mentioned\n",
        "    8. title: The title of the article or document (if applicable)\n",
        "    9. author: The name of the author(s) (if mentioned)\n",
        "    10. citations: Any references, citations, or hyperlinks\n",
        "    11. sentiment: Overall sentiment of the chunk (positive, negative, or neutral)\n",
        "    12. key_sentences: 1-2 important sentences that summarize main points\n",
        "    13. entity_relationships: Relationships between entities in the format {{\"subject\": \"entity1\", \"relationship\": \"verb\", \"object\": \"entity2\"}}\n",
        "\n",
        "    You are part of a bigger system, and you must always respond with a pure JSON response. The system will break if you don't. NEVER add a JSON code block syntax to your response.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that analyzes text chunks and extracts detailed metadata.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    return json.loads(response.choices[0].message.content)\n",
        "\n",
        "def process_document(text: str, chunk_size: int = 10) -> List[Dict]:\n",
        "    \"\"\"Process the entire document.\"\"\"\n",
        "    cleaned_text = clean_text(text)\n",
        "    sentences = split_into_sentences(cleaned_text)\n",
        "    chunks = create_chunks(sentences, chunk_size)\n",
        "\n",
        "    global_context = get_initial_context(cleaned_text)\n",
        "\n",
        "    processed_chunks = []\n",
        "    for chunk in chunks:\n",
        "        chunk_metadata = process_chunk(chunk, global_context, processed_chunks)\n",
        "        processed_chunks.append({\n",
        "            \"original_text\": chunk,\n",
        "            \"metadata\": chunk_metadata\n",
        "        })\n",
        "\n",
        "    return processed_chunks\n",
        "\n",
        "\n",
        "def get_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"Get embedding for a given text using OpenAI's API.\"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=text\n",
        "    )\n",
        "    return np.array(response.data[0].embedding)\n",
        "\n",
        "def create_chunked_vector_db(processed_chunks: List[Dict]) -> tuple:\n",
        "    embeddings = []\n",
        "    for chunk in processed_chunks:\n",
        "        # Convert metadata dictionary to a string\n",
        "        metadata_str = json.dumps(chunk)\n",
        "        embedding = get_embedding(metadata_str)\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "    embeddings = np.array(embeddings)\n",
        "    embeddings = normalize(embeddings)\n",
        "\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    return index, embeddings\n",
        "\n",
        "def create_fulltext_vector_db(text: str) -> tuple:\n",
        "    \"\"\"Create a vector database from the full text.\"\"\"\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    embeddings = [get_embedding(sentence) for sentence in sentences]\n",
        "    embeddings = np.array(embeddings)\n",
        "    embeddings = normalize(embeddings)\n",
        "\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    return index, embeddings, sentences\n",
        "\n",
        "# Update the perform_search function to handle both chunked and full-text cases\n",
        "def perform_search(query: str, index: faiss.IndexFlatL2, embeddings: np.ndarray, data: List[Dict] = None, k: int = 5) -> List[tuple]:\n",
        "    query_vector = get_embedding(query)\n",
        "    query_vector = normalize(query_vector.reshape(1, -1))\n",
        "\n",
        "    distances, indices = index.search(query_vector, k)\n",
        "    results = []\n",
        "    for idx, distance in zip(indices[0], distances[0]):\n",
        "        if data:  # For chunked DB\n",
        "            metadata = data[idx]['metadata']\n",
        "            results.append((idx, distance, metadata))\n",
        "        else:  # For full-text DB\n",
        "            results.append((idx, distance))\n",
        "    return results\n",
        "\n",
        "# Update the comparison function\n",
        "def compare_performance(processed_chunks: List[Dict], full_text: str, queries: List[str]):\n",
        "    chunked_db, chunked_embeddings = create_chunked_vector_db(processed_chunks)\n",
        "    fulltext_db, fulltext_embeddings, sentences = create_fulltext_vector_db(full_text)\n",
        "\n",
        "    print(\"Comparison Results:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"Query: {query}\")\n",
        "\n",
        "        chunked_results = perform_search(query, chunked_db, chunked_embeddings, processed_chunks)\n",
        "        print(\"Chunked DB - Top 5 results:\")\n",
        "        for idx, score, metadata in chunked_results:\n",
        "            print(f\"Chunk {idx} (Score: {score:.4f}):\")\n",
        "            print(f\"Concepts: {metadata['concepts']}\")\n",
        "            print(f\"Key Sentences: {metadata['key_sentences']}\")\n",
        "\n",
        "        fulltext_results = perform_search(query, fulltext_db, fulltext_embeddings)\n",
        "        print(\"\\nFull-text DB - Top 5 results:\")\n",
        "        for idx, score in fulltext_results:\n",
        "            print(f\"Sentence {idx} (Score: {score:.4f}): {sentences[idx]}\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "# The main function remains the same\n",
        "\n",
        "def main():\n",
        "    print(\"Please upload a PDF file.\")\n",
        "    file_name = upload_file()\n",
        "\n",
        "    print(f\"Processing {file_name}...\")\n",
        "    text = read_pdf(file_name)\n",
        "\n",
        "    processed_chunks = process_document(text)\n",
        "\n",
        "    output_file = f\"{os.path.splitext(file_name)[0]}_processed.json\"\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(processed_chunks, f, indent=2)\n",
        "\n",
        "    print(f\"Processing complete. Results saved to {output_file}\")\n",
        "    files.download(output_file)\n",
        "\n",
        "    # Perform vector database comparison\n",
        "    queries = [\n",
        "        \"What is the main topic of the document?\",\n",
        "        \"Who are the key people mentioned?\",\n",
        "        \"What are the main conclusions or findings?\"\n",
        "    ]\n",
        "\n",
        "    compare_performance(processed_chunks, text, queries)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}